{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pillow torch diffusers google-cloud-storage langchain pinecone-client google-generativeai serpapi google-colab > /dev/null 2>&1\n"
      ],
      "metadata": {
        "id": "qw-hNVM6RJki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client > /dev/null 2>&1\n",
        "!pip install tiktoken > /dev/null 2>&1\n",
        "!pip install google-search-results > /dev/null 2>&1\n",
        "!pip install -U langchain langchain-community > /dev/null 2>&1\n",
        "\n"
      ],
      "metadata": {
        "id": "NLD-FsZMSSy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install fastapi uvicorn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bC5M7t9BTvDk",
        "outputId": "730f7835-1b23-4436-8aac-63f3770fe3dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: uvicorn, starlette, fastapi\n",
            "Successfully installed fastapi-0.115.8 starlette-0.45.3 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-multipart\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS3zmcyQT2j3",
        "outputId": "79f9113d-55d3-475b-afe0-33505db9ffd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-multipart\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: python-multipart\n",
            "Successfully installed python-multipart-0.0.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest_asyncio\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSvUqMvyVq46",
        "outputId": "75542344-9e18-4c87-b7a1-124a19d8ef3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0md2HcSHDQGc",
        "outputId": "8401a31b-8bd0-415b-de20-2ac9d8df91d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "import os\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Add your ngrok authentication token\n",
        "!ngrok authtoken 2pUNMdn5heZDqr50ndLJ9hFA348_7BNec91SNvazozmDtzkf5\n",
        "\n",
        "# Set the environment variable\n",
        "#os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/encoded-blend-439905-g7-3deac22ca1bf.json\"\n",
        "\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/drive/MyDrive/service_account_gcp/encoded-blend-439905-g7-3deac22ca1bf.json\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7y96-joyR3g",
        "outputId": "13757cd4-c198-4f4e-d26f-c382c8021eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn PyPDF2 python-docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOsr8Zqi5Mwt",
        "outputId": "dfbf9239-423c-438d-91bd-6e29c0915e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.115.8)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.34.0)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.45.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1 python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytesseract\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W0gdPznPaYkZ",
        "outputId": "e93f1134-aa14-4778-af06-3b68ca0381fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (24.2)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.1.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: pytesseract\n",
            "Successfully installed pytesseract-0.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "wviiMhoc6m2m",
        "outputId": "3d6b305c-c957-49b4-b298-039b6f1d6180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok tunnel public URL: NgrokTunnel: \"https://c6d4-35-243-160-189.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [1133]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2001:468:300:800:f0b4:68cf:8f71:8aa6:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     2001:468:300:800:f0b4:68cf:8f71:8aa6:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     2001:468:300:800:f0b4:68cf:8f71:8aa6:0 - \"POST /submit HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [1133]\n"
          ]
        }
      ],
      "source": [
        "import io\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from fastapi import FastAPI, UploadFile, File, Form\n",
        "from fastapi.responses import HTMLResponse\n",
        "from typing import List, Optional\n",
        "import uvicorn\n",
        "import PyPDF2\n",
        "import docx\n",
        "from tempfile import NamedTemporaryFile\n",
        "from pyngrok import ngrok\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "\n",
        "# Configure Google Gemini API (Replace with your actual API Key)\n",
        "GOOGLE_API_KEY = \"AIzaSyCLK0Rrxp3ND7YIVmuMLdwtmUbOYAo5IVc\"\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Function to extract text from PDFs\n",
        "def extract_text_from_pdf(file_bytes: bytes) -> str:\n",
        "    \"\"\"Extract text from a PDF file given its bytes.\"\"\"\n",
        "    pdf_reader = PyPDF2.PdfReader(io.BytesIO(file_bytes))\n",
        "    text = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            text += page_text\n",
        "    return text\n",
        "\n",
        "# Function to extract text from Word files\n",
        "def extract_text_from_docx(file_bytes: bytes) -> str:\n",
        "    \"\"\"Extract text from a Word document given its bytes.\"\"\"\n",
        "    with NamedTemporaryFile(delete=False, suffix=\".docx\") as tmp:\n",
        "        tmp.write(file_bytes)\n",
        "        tmp.flush()\n",
        "        document = docx.Document(tmp.name)\n",
        "    text = \"\\n\".join([para.text for para in document.paragraphs])\n",
        "    return text\n",
        "\n",
        "# Function to extract text from images using OCR (Tesseract)\n",
        "def extract_text_from_image(file_bytes: bytes) -> str:\n",
        "    \"\"\"Extract text from an image using Tesseract OCR.\"\"\"\n",
        "    try:\n",
        "        image = Image.open(io.BytesIO(file_bytes))\n",
        "        text = pytesseract.image_to_string(image)\n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        return f\"[Error processing image: {str(e)}]\"\n",
        "\n",
        "# Function to call Google Gemini API\n",
        "def call_gemini_api(prompt: str) -> str:\n",
        "    \"\"\"Send a prompt to Gemini API and return the response.\"\"\"\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "    response = model.generate_content([prompt])\n",
        "    return response.text\n",
        "\n",
        "@app.get(\"/\", response_class=HTMLResponse)\n",
        "async def read_form():\n",
        "    \"\"\"Render the HTML form for user input.\"\"\"\n",
        "    html_content = \"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Prompt & File Upload</title>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Enter Prompt and Upload Files</h1>\n",
        "        <form action=\"/submit\" method=\"post\" enctype=\"multipart/form-data\">\n",
        "            <label for=\"prompt\">Prompt:</label><br>\n",
        "            <textarea name=\"prompt\" rows=\"4\" cols=\"50\" placeholder=\"Enter your prompt here\"></textarea><br><br>\n",
        "            <label for=\"files\">Upload PDF, Word, or Image Files:</label><br>\n",
        "            <input type=\"file\" name=\"files\" multiple><br><br>\n",
        "            <input type=\"submit\" value=\"Submit\">\n",
        "        </form>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return html_content\n",
        "\n",
        "@app.post(\"/submit\", response_class=HTMLResponse)\n",
        "async def submit_form(\n",
        "    prompt: str = Form(...),\n",
        "    files: Optional[List[UploadFile]] = File([])\n",
        "):\n",
        "    \"\"\"Process user input and send extracted content to Gemini API.\"\"\"\n",
        "\n",
        "    extracted_text = \"\"\n",
        "    for file in files:\n",
        "        content = await file.read()\n",
        "        if file.content_type == \"application/pdf\":\n",
        "            extracted_text += \"\\n\" + extract_text_from_pdf(content)\n",
        "        elif file.content_type in [\n",
        "            \"application/vnd.openxmlformats-officedocument.wordprocessingml.document\",\n",
        "            \"application/msword\"\n",
        "        ]:\n",
        "            extracted_text += \"\\n\" + extract_text_from_docx(content)\n",
        "        elif file.content_type in [\"image/jpeg\", \"image/png\", \"image/jpg\"]:\n",
        "            extracted_text += \"\\n\" + extract_text_from_image(content)\n",
        "        else:\n",
        "            extracted_text += f\"\\n[Unsupported file type: {file.filename}]\"\n",
        "\n",
        "    # Combine the user prompt with extracted file text\n",
        "    combined_prompt = f\"Analyze the initial price of an iPhone and forecast its price six months from now using the provided images. Extract sentiment analysis values related to the iPhone from the second image. Incorporate predicted iPhone price data from the forecasted values in the images. Additionally, search the web for relevant information on supply chain disruptions and current import/export exchange rate duties in the U.S. market.Using these inputs, develop a mathematical reasoning model that identifies factors directly proportional to iPhone price fluctuations. Based on this model, account for economic, market, and sentiment-driven factors to predict the iPhone price for the specified period from the following extracted text from images and if any additional details from prompt2. Here is Prompt 2 if any {prompt} Extracted Content from Images \\n\\nExtracted Content:\\n{extracted_text}\"\n",
        "\n",
        "    # Call Gemini API\n",
        "    gemini_response = call_gemini_api(combined_prompt)\n",
        "\n",
        "    # Display the result\n",
        "    html_response = f\"\"\"\n",
        "    <!DOCTYPE html>\n",
        "    <html>\n",
        "    <head>\n",
        "        <title>Gemini AI Response</title>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Gemini AI Response</h1>\n",
        "        <p><strong>Input Prompt:</strong></p>\n",
        "        <pre>{prompt}</pre>\n",
        "        <p><strong>Gemini Response:</strong></p>\n",
        "        <pre>{gemini_response}</pre>\n",
        "        <br>\n",
        "        <a href=\"/\">Back to Form</a>\n",
        "    </body>\n",
        "    </html>\n",
        "    \"\"\"\n",
        "    return html_response\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Open an ngrok tunnel for public access\n",
        "    public_url = ngrok.connect(8000)\n",
        "    print(\"ngrok tunnel public URL:\", public_url)\n",
        "\n",
        "    # Start the Uvicorn server\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FkedSnTaTyoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai # Installing Gemini AI for use with API\n",
        "\n",
        "# Google Gemini AI (API)\n",
        "import google.generativeai as genai\n",
        "\n",
        "# File management\n",
        "import os\n",
        "\n",
        "# Ff we call urls\n",
        "import requests\n",
        "\n",
        "# Handle images\n",
        "from IPython.display import Image\n",
        "\n",
        "# Use this if you set the environment variable for the api key: (preferred route when not using notebook)\n",
        "# genai.configure(api_key=os.environ[\"API_KEY\"])\n",
        "\n",
        "# If using File From Colab Notebook Directly, use stored API key\n",
        "from google.colab import userdata\n",
        "api=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "# Initializing your key for use\n",
        "genai.configure(api_key=api)\n",
        "\n",
        "model = genai.GenerativeModel('models/gemini-1.5-pro-001')"
      ],
      "metadata": {
        "id": "FZIbhUapgtAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of Available Models: NOT ALL WORK because of deprecation\n",
        "for model_name in genai.list_models():\n",
        "    if 'generateContent'  in model_name.supported_generation_methods:\n",
        "        print(model_name.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 601
        },
        "id": "uLm64g2agwLs",
        "outputId": "b081ed6b-ab95-4b91-9a91-d4eb575164f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "models/gemini-1.0-pro-latest\n",
            "models/gemini-1.0-pro\n",
            "models/gemini-pro\n",
            "models/gemini-1.0-pro-001\n",
            "models/gemini-1.0-pro-vision-latest\n",
            "models/gemini-pro-vision\n",
            "models/gemini-1.5-pro-latest\n",
            "models/gemini-1.5-pro-001\n",
            "models/gemini-1.5-pro-002\n",
            "models/gemini-1.5-pro\n",
            "models/gemini-1.5-flash-latest\n",
            "models/gemini-1.5-flash-001\n",
            "models/gemini-1.5-flash-001-tuning\n",
            "models/gemini-1.5-flash\n",
            "models/gemini-1.5-flash-002\n",
            "models/gemini-1.5-flash-8b\n",
            "models/gemini-1.5-flash-8b-001\n",
            "models/gemini-1.5-flash-8b-latest\n",
            "models/gemini-1.5-flash-8b-exp-0827\n",
            "models/gemini-1.5-flash-8b-exp-0924\n",
            "models/gemini-2.0-flash-exp\n",
            "models/gemini-2.0-flash\n",
            "models/gemini-2.0-flash-001\n",
            "models/gemini-2.0-flash-lite-preview\n",
            "models/gemini-2.0-flash-lite-preview-02-05\n",
            "models/gemini-2.0-pro-exp\n",
            "models/gemini-2.0-pro-exp-02-05\n",
            "models/gemini-exp-1206\n",
            "models/gemini-2.0-flash-thinking-exp-01-21\n",
            "models/gemini-2.0-flash-thinking-exp\n",
            "models/gemini-2.0-flash-thinking-exp-1219\n",
            "models/learnlm-1.5-pro-experimental\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "from PIL import Image\n",
        "\n",
        "# Step 1: Upload Files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Step 2: Store Filenames Dynamically\n",
        "file_names = list(uploaded.keys())\n",
        "\n",
        "# Step 3: Open Images Dynamically\n",
        "images = [Image.open(file_name) for file_name in file_names]\n",
        "\n",
        "# Step 4: Assign Image Variables Dynamically (Optional)\n",
        "image_dict = {f\"img_{i+1}\": img for i, img in enumerate(images)}\n",
        "\n",
        "# Print Assigned Variables\n",
        "for var_name, img in image_dict.items():\n",
        "    print(f\"{var_name} loaded from file: {img.filename}\")\n",
        "\n",
        "# Example: Access Images\n",
        "img_1 = image_dict[\"img_1\"]\n",
        "img_2 = image_dict[\"img_2\"]\n",
        "img_3 = image_dict[\"img_3\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "REkj5wzngyNj",
        "outputId": "25d96091-5f09-4c6a-e7c0-6ad05b1663f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e87c6b5c-55c9-49ce-8658-cc3db5325dee\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e87c6b5c-55c9-49ce-8658-cc3db5325dee\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving inflation_price.png to inflation_price (1).png\n",
            "Saving model_price.jpg to model_price (1).jpg\n",
            "Saving sentiment.jpg to sentiment (1).jpg\n",
            "img_1 loaded from file: inflation_price (1).png\n",
            "img_2 loaded from file: model_price (1).jpg\n",
            "img_3 loaded from file: sentiment (1).jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gowUBwSVhlAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from google.colab import files\n",
        "from PIL import Image\n",
        "\n",
        "# Step 1: Upload Files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Step 2: Store Filenames Dynamically\n",
        "file_names = list(uploaded.keys())\n",
        "\n",
        "# Step 3: Open Images Dynamically\n",
        "images = [Image.open(file_name) for file_name in file_names]\n",
        "\n",
        "# Step 4: Initialize Gemini Model\n",
        "model = genai.GenerativeModel('gemini-2.0-flash')\n",
        "\n",
        "# Step 5: Define Prompt\n",
        "prompt = \"Analyze the initial price of an iPhone and forecast its price six months from now using the provided images. Extract sentiment analysis values related to the iPhone from the second image. Incorporate predicted iPhone price data from the forecasted values in the images. Additionally, search the web for relevant information on supply chain disruptions and current import/export exchange rate duties in the U.S. market.Using these inputs, develop a mathematical reasoning model that identifies factors directly proportional to iPhone price fluctuations. Based on this model, account for economic, market, and sentiment-driven factors to predict the iPhone price for the specified period from the following extracted text from images. Also say if the price of the phone has decreased or increased and the reason for its increase or decrease \"\n",
        "\n",
        "# Step 6: Send Each Image Separately Along with the Prompt\n",
        "result = model.generate_content([*images, prompt])  # Correct way to pass multiple images\n",
        "\n",
        "# Step 7: Output Response\n",
        "print(result.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hfZqWBdng16-",
        "outputId": "69d383fa-51ad-4d1c-bf9d-4d325e835618"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d9c7dcae-83d6-4387-ace7-97fead06104d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d9c7dcae-83d6-4387-ace7-97fead06104d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving inflation_price.png to inflation_price.png\n",
            "Saving model_price.jpg to model_price.jpg\n",
            "Saving sentiment.jpg to sentiment.jpg\n",
            "Okay, this is a complex task that requires combining information from images, external data, and reasoning. Here's a breakdown of my approach and findings:\n",
            "\n",
            "**1. Image Analysis & Data Extraction:**\n",
            "\n",
            "*   **Image 1 (Price Prediction Dashboard):**\n",
            "\n",
            "    *   **Initial Price:** February 2025: $808.83\n",
            "    *   **Predicted Price (April 2025):** $828.85\n",
            "    *   **Economic Data:** Inflation Rate: 4.116%, GDP Growth: 2.887%\n",
            "*   **Image 2 (Historical Data & Prediction):**\n",
            "\n",
            "    *   This is a visual of historical iPhone data (likely iPhone 12 mini).  Crucially, it also shows a 6-month *prediction* using historical data only.\n",
            "    *   **Sentiment Analysis (Embedded in Prediction Summary):**\n",
            "        *   Average Predicted Price (next 6 months from image data): $263.42\n",
            "        *   Maximum Predicted Price: $293.08\n",
            "        *   Minimum Predicted Price: $232.04\n",
            "*   **Image 3 (High Activity on r/deals with Reasons):**\n",
            "\n",
            "    *   **Sentiment Analysis (from Reddit data):** This image shows correlations between Reddit post activity (assumed as a proxy for consumer interest/sentiment) and time periods associated with sales events or product launches. High activity generally corresponds with significant events that may impact demand or supply.\n",
            "    *   Examples from the image include:\n",
            "        *   \"Apple Sale Event,\" \"Amazon Prime Day,\" \"Black Friday\"\n",
            "        *   Indicates sales events where the iPhone is frequently featured or available.\n",
            "\n",
            "**2. Web Search and External Data:**\n",
            "\n",
            "*   **Supply Chain Disruptions:**  (General Search)\n",
            "    *   *Disclaimer: I cannot conduct real-time web searches*.  However, current search results would need to identify any news regarding disruptions in the iPhone supply chain. The key indicators would be factory shutdowns in China, component shortages, shipping delays, and political/trade tensions affecting the supply chain.\n",
            "*   **U.S. Import/Export Exchange Rate Duties:** (General Search)\n",
            "    *   *Disclaimer: I cannot conduct real-time web searches*. Focus would be on:\n",
            "        *   Current tariffs on electronic goods imported from countries where iPhones are assembled (mainly China).\n",
            "        *   Exchange rates between USD and relevant currencies (e.g., CNY).  A stronger USD would generally make imports cheaper (all else being equal).\n",
            "\n",
            "**3. Mathematical Reasoning Model**\n",
            "\n",
            "Let's create a simplified model, acknowledging that a real-world model would be far more complex:\n",
            "\n",
            "*   **Price (P) = Base Price + (Factor1) + (Factor2) + (Factor3)**\n",
            "\n",
            "    *   *Base Price:* Initial Price ($808.83)\n",
            "    *   *Factor 1: Economic Factors* = (Inflation Rate * Sensitivity Factor) + (GDP Growth Rate * Sensitivity Factor)\n",
            "        *   Inflation Rate = 4.116%\n",
            "        *   GDP Growth = 2.887%\n",
            "        *   *Sensitivity Factors:* These constants reflect how responsive iPhone prices are to economic changes. We'll assign a sensitivity factor (SF) for each. (This is a highly simplified assumption).\n",
            "        *   Let's assume Sensitivity Factors for both are 0.5 (50% correlation to the base price), so SF Inflation=SF Growth=0.5\n",
            "        *   Factor 1:  4.116% \\* 0.5 + 2.887% \\* 0.5 = 0.02058+0.014435 = 0.035015 = 3.5%\n",
            "        *   Inflation increases the prices of the iPhone.\n",
            "    *   *Factor 2: Market Factors* = (Demand - Supply) * Market Sensitivity Factor\n",
            "        *   *Demand:* We'll use the Reddit activity as a rough proxy. Assume that increased activity is correlated with increased demand.\n",
            "        *   *Supply:* Assess for supply chain issues (from search).  Let's assume we discover that supply chain disruptions *are* present and are likely to decrease supply.\n",
            "        *   *Market Sensitivity Factor:* We will assume 0.4, 40% correlation with the base price\n",
            "        *   Let's assume (Demand-Supply) increased by 5%, (Demand-Supply) = 0.05\n",
            "        *   Factor 2: 0.05 \\* 0.4 = 0.02 = 2%\n",
            "        *   Increase in demand and decrease in Supply increases the prices of the iPhone.\n",
            "    *   *Factor 3: Sentiment Factors* = (Average Predicted Price From Historical Data - Base Price) * Sensitivity Factor\n",
            "        *   (263.42 - 808.83) \\* 0.2\n",
            "        *   -545.41 \\* 0.2 = -109.082\n",
            "        *   Negative Sentiment decreases the prices of the iPhone\n",
            "\n",
            "*   **Calculations:**\n",
            "\n",
            "    *   Factor 1: (0.04116\\*0.5)+(0.02887*0.5)=.035 = ~3.5%\n",
            "    *   Factor 2 = 2%\n",
            "    *   Factor 3: -109.082\n",
            "    *   P = 808.83 + (808.83\\*.035) + (808.83\\*.02) + (-109.082)\n",
            "    *   P = 808.83 + 28.31 + 16.18 + (-109.082)\n",
            "    *   P = 744.25\n",
            "\n",
            "*   **Predicted Price:** $744.25\n",
            "\n",
            "**4. Final Prediction and Explanation**\n",
            "\n",
            "*   **Predicted iPhone Price (Six Months):** $744.25\n",
            "*   **Change:** The model predicts a decrease in the price of the phone from approximately $808.83 to $744.25\n",
            "\n",
            "**Reasoning for the Decrease:**\n",
            "\n",
            "*   **Sentiment from Historical Data:** The 6-Month Price Prediction Summary from the image has a very low average price. In the past months the phone price has significantly decreased and the predicted prices is a continuing of that decrease.\n",
            "\n",
            "**Important Considerations and Limitations:**\n",
            "\n",
            "*   **Simplifications:**  The model is a very simple representation.  It does not account for the complex interactions of factors, such as brand loyalty, marketing campaigns, competitive pricing from other manufacturers, or technological innovations.\n",
            "*   **Sensitivity Factors:**  The sensitivity factors were *arbitrarily* assigned.  Determining accurate sensitivity factors would require significant statistical analysis of historical data.\n",
            "*   **Data Quality:**  The Reddit data is a rough proxy for demand.  It's not a precise measure. The historical prices may reflect a much older version of the iPhone and be outdated.\n",
            "*   **External Data Accuracy:** The predictions depend heavily on the accuracy and timeliness of the external data collected (supply chain news, tariff information).\n",
            "*   **Specific Model of iPhone:** It is unclear which specific model this forecast applies to. This makes a big difference.\n",
            "\n",
            "**In Conclusion:**\n",
            "\n",
            "The price of the phone appears to be set to decrease due to the combination of economic, market and negative sentiment factors. The historical data of decreasing prices play a significant role in the forecast.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_=Image('IMG_7850.jpg') # my file for example\n"
      ],
      "metadata": {
        "id": "nNSqdoxvg-Gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-pro-001')\n",
        "\n",
        "# Text prompt for Gemini AI\n",
        "prompt='analyze and extract all information from image, including best by date and all other information'\n",
        "\n",
        "#Gemini takes our img, prompt and generate a response string from it\n",
        "result=model.generate_content([img_,prompt])\n",
        "\n",
        "\n",
        "# Output text\n",
        "print(result.text)"
      ],
      "metadata": {
        "id": "Sz-8lTxohCgd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}